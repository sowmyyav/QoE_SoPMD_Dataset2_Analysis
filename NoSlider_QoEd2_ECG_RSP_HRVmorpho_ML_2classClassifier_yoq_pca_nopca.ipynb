{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from numpy.random import seed\n",
    "seed(0)\n",
    "tf.random.set_seed(0)\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Dropout \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import imblearn\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Permute\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import LSTM, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking\n",
    "#from keras.utils import plot_model\n",
    "import sklearn\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Activation\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras import optimizers \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV \n",
    "\n",
    "from scipy.stats import expon\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import gamma\n",
    "from sklearn import metrics\n",
    "from scipy.stats import reciprocal\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load peripheral preprocessed data\n",
    "#rsp_df = joblib.load(open('/home/sowmya/QoE_D2/Dataset2/SpyderCodeDataset2/data/rsp_18pyhrvftr_d2.dat', 'rb'))\n",
    "#rsp_data= np.array(rsp_df)\n",
    "rsp_df = joblib.load(open('C:/Users/Sowmya/QoE/QoE_D2/Dataset2/SpyderCodeDataset2/data/rsp_40features.dat', 'rb'))\n",
    "rsp_data= rsp_df.drop(['RRV_SampEn'], axis =1)\n",
    "np.count_nonzero(np.isinf(rsp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load peripheral preprocessed data\n",
    "ecg_HRV, ratings = joblib.load(open('C:/Users/Sowmya/QoE/QoE_D2/Dataset2/SpyderCodeDataset2/data/ECG_18pyHRVfeatures_new.dat', 'rb'))\n",
    "ecg_morpho, ratings = joblib.load(open('C:/Users/Sowmya/QoE/QoE_D2/Dataset2/SpyderCodeDataset2/data/ECG_morphofeatures.dat', 'rb'))\n",
    "\n",
    "# Place the pyHRV and Morphological ff features DataFrames side by side\n",
    "horizontal_stack = pd.concat([ecg_HRV, ecg_morpho, rsp_data], axis=1)\n",
    "#drop columns with all NAN values\n",
    "ff_df = horizontal_stack.dropna(axis=1, how='all')\n",
    "ff_data= np.array(ff_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if input contains zero and infinity or a value too large\n",
    "np.count_nonzero(np.isnan(ff_data))\n",
    "np.count_nonzero(np.isinf(ff_data))\n",
    "\n",
    "#Indices containing inf values and convert them to NaN\n",
    "indsinf = np.where(np.isinf(ff_data))\n",
    "ff_data[indsinf] = 'NaN'\n",
    "\n",
    "#Replace NaN values with mean of the column\n",
    "col_mean = np.nanmean(ff_data, axis=0)    \n",
    "inds = np.where(np.isnan(ff_data))\n",
    "ff_data[inds] = np.take(col_mean, inds[1])\n",
    "np.count_nonzero(np.isnan(ff_data))\n",
    "np.count_nonzero(np.isinf(ff_data))\n",
    "\n",
    "#ff= pd.DataFrame(data= ff_data, columns= ff_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 317, 1: 223})\n"
     ]
    }
   ],
   "source": [
    "#load ratings from excel sheet as pandas dataframe\n",
    "load_ratings = pd.read_excel ('C:/Users/Sowmya/QoE/QoE_D2/Dataset2/Ratings_Dataset2.xlsx', sheet_name='EI_ALL')\n",
    "ratings1 = load_ratings.drop(['subject'], axis=1) #delete subject from dataframe\n",
    "ff_label = ratings1.values #convert dataframe to numpy array\n",
    "\n",
    "def data_binarizer_two(ratings):\n",
    "\t\"\"\"binarizes the data below and above the threshold\"\"\"\n",
    "\tbinarized = []\n",
    "\tfor rating in ratings:\n",
    "\t\tif rating <= 6:\n",
    "\t\t\tbinarized.append(0)\n",
    "\t\telse:\n",
    "\t\t\tbinarized.append(1)\n",
    "\treturn binarized\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "y_ia = np.array(data_binarizer_two([el[0] for el in ff_label]))\n",
    "y_aq = np.array(data_binarizer_two([el[1] for el in ff_label]))\n",
    "y_iv = np.array(data_binarizer_two([el[2] for el in ff_label]))\n",
    "y_il = np.array(data_binarizer_two([el[3] for el in ff_label]))\n",
    "y_oq = np.array(data_binarizer_two([el[4] for el in ff_label]))\n",
    "y_sa = np.array(data_binarizer_two([el[5] for el in ff_label]))\n",
    "\n",
    "counter = Counter(y_il)\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.65        39\n",
      "           1       0.79      0.84      0.82        69\n",
      "\n",
      "    accuracy                           0.76       108\n",
      "   macro avg       0.74      0.73      0.73       108\n",
      "weighted avg       0.76      0.76      0.76       108\n",
      "\n",
      "SVM Accuracy: 0.759259\n",
      "SVM F1score: 0.756143\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.26      0.36        39\n",
      "           1       0.68      0.91      0.78        69\n",
      "\n",
      "    accuracy                           0.68       108\n",
      "   macro avg       0.65      0.58      0.57       108\n",
      "weighted avg       0.66      0.68      0.63       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.675926\n",
      "SVM RandomCV F1score: 0.631313\n",
      "{'C': 156.88961399691684, 'class_weight': None, 'gamma': 0.06538825690914733, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.72      0.63        39\n",
      "           1       0.81      0.68      0.74        69\n",
      "\n",
      "    accuracy                           0.69       108\n",
      "   macro avg       0.69      0.70      0.68       108\n",
      "weighted avg       0.72      0.69      0.70       108\n",
      "\n",
      "KNN Accuracy grid search: 0.694444\n",
      "KNN F1score grid search: 0.700094\n",
      "{'leaf_size': 1, 'n_neighbors': 3, 'p': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.69      0.64        39\n",
      "           1       0.81      0.72      0.76        69\n",
      "\n",
      "    accuracy                           0.71       108\n",
      "   macro avg       0.70      0.71      0.70       108\n",
      "weighted avg       0.73      0.71      0.72       108\n",
      "\n",
      "KNN Accuracy: 0.712963\n",
      "KNN F1score: 0.717113\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64        39\n",
      "           1       0.80      0.80      0.80        69\n",
      "\n",
      "    accuracy                           0.74       108\n",
      "   macro avg       0.72      0.72      0.72       108\n",
      "weighted avg       0.74      0.74      0.74       108\n",
      "\n",
      "RF Accuracy: 0.740741\n",
      "RF F1score: 0.740741\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        39\n",
      "           1       0.84      0.81      0.82        69\n",
      "\n",
      "    accuracy                           0.78       108\n",
      "   macro avg       0.76      0.76      0.76       108\n",
      "weighted avg       0.78      0.78      0.78       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.777778\n",
      "RF Random F1score _PARAMS: 0.778922\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65        39\n",
      "           1       0.80      0.81      0.81        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.73      0.73      0.73       108\n",
      "weighted avg       0.75      0.75      0.75       108\n",
      "\n",
      "RF Random Accuracy: 0.75\n",
      "RF Random F1score: 0.749276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.69      0.64        39\n",
      "           1       0.81      0.72      0.76        69\n",
      "\n",
      "    accuracy                           0.71       108\n",
      "   macro avg       0.70      0.71      0.70       108\n",
      "weighted avg       0.73      0.71      0.72       108\n",
      "\n",
      "KNN Accuracy: 0.712963\n",
      "KNN F1score: 0.717113\n",
      "{'n_neighbors': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "'''\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "'''\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69        39\n",
      "           1       0.85      0.74      0.79        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.74      0.75      0.74       108\n",
      "weighted avg       0.77      0.75      0.75       108\n",
      "\n",
      "SVM Accuracy: 0.75\n",
      "SVM F1score: 0.75421\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.46      0.51        39\n",
      "           1       0.73      0.81      0.77        69\n",
      "\n",
      "    accuracy                           0.69       108\n",
      "   macro avg       0.65      0.64      0.64       108\n",
      "weighted avg       0.67      0.69      0.68       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.685185\n",
      "SVM RandomCV F1score: 0.675821\n",
      "{'C': 43.434565697036696, 'class_weight': 'balanced', 'gamma': 0.6765393734039725, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.53        39\n",
      "           1       0.73      0.71      0.72        69\n",
      "\n",
      "    accuracy                           0.65       108\n",
      "   macro avg       0.62      0.62      0.62       108\n",
      "weighted avg       0.65      0.65      0.65       108\n",
      "\n",
      "KNN Accuracy grid search: 0.648148\n",
      "KNN F1score grid search: 0.649959\n",
      "{'leaf_size': 1, 'n_neighbors': 1, 'p': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.53        39\n",
      "           1       0.73      0.71      0.72        69\n",
      "\n",
      "    accuracy                           0.65       108\n",
      "   macro avg       0.62      0.62      0.62       108\n",
      "weighted avg       0.65      0.65      0.65       108\n",
      "\n",
      "KNN Accuracy: 0.648148\n",
      "KNN F1score: 0.649959\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.62      0.63        39\n",
      "           1       0.79      0.81      0.80        69\n",
      "\n",
      "    accuracy                           0.74       108\n",
      "   macro avg       0.72      0.71      0.72       108\n",
      "weighted avg       0.74      0.74      0.74       108\n",
      "\n",
      "RF Accuracy: 0.740741\n",
      "RF F1score: 0.739181\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.51      0.54        39\n",
      "           1       0.74      0.78      0.76        69\n",
      "\n",
      "    accuracy                           0.69       108\n",
      "   macro avg       0.66      0.65      0.65       108\n",
      "weighted avg       0.68      0.69      0.68       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.685185\n",
      "RF Random F1score _PARAMS: 0.681111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.59      0.59        39\n",
      "           1       0.77      0.77      0.77        69\n",
      "\n",
      "    accuracy                           0.70       108\n",
      "   macro avg       0.68      0.68      0.68       108\n",
      "weighted avg       0.70      0.70      0.70       108\n",
      "\n",
      "RF Random Accuracy: 0.703704\n",
      "RF Random F1score: 0.703704\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=5)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.46      0.55        39\n",
      "           1       0.74      0.88      0.81        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.72      0.67      0.68       108\n",
      "weighted avg       0.73      0.73      0.72       108\n",
      "\n",
      "SVM Accuracy: 0.731481\n",
      "SVM F1score: 0.716188\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.38      0.49        39\n",
      "           1       0.72      0.90      0.80        69\n",
      "\n",
      "    accuracy                           0.71       108\n",
      "   macro avg       0.70      0.64      0.65       108\n",
      "weighted avg       0.71      0.71      0.69       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.712963\n",
      "SVM RandomCV F1score: 0.688707\n",
      "{'C': 85.8392964354515, 'class_weight': 'balanced', 'gamma': 0.22637335586430757, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.69      0.65        39\n",
      "           1       0.81      0.75      0.78        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.71      0.72      0.72       108\n",
      "weighted avg       0.74      0.73      0.73       108\n",
      "\n",
      "KNN Accuracy grid search: 0.731481\n",
      "KNN F1score grid search: 0.734522\n",
      "{'leaf_size': 1, 'n_neighbors': 1, 'p': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.69      0.65        39\n",
      "           1       0.81      0.75      0.78        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.71      0.72      0.72       108\n",
      "weighted avg       0.74      0.73      0.73       108\n",
      "\n",
      "KNN Accuracy: 0.731481\n",
      "KNN F1score: 0.734522\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.54      0.55        39\n",
      "           1       0.74      0.75      0.75        69\n",
      "\n",
      "    accuracy                           0.68       108\n",
      "   macro avg       0.65      0.65      0.65       108\n",
      "weighted avg       0.67      0.68      0.67       108\n",
      "\n",
      "RF Accuracy: 0.675926\n",
      "RF F1score: 0.674987\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.58        39\n",
      "           1       0.76      0.83      0.79        69\n",
      "\n",
      "    accuracy                           0.72       108\n",
      "   macro avg       0.70      0.68      0.69       108\n",
      "weighted avg       0.72      0.72      0.72       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.722222\n",
      "RF Random F1score _PARAMS: 0.716435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56        39\n",
      "           1       0.75      0.78      0.77        69\n",
      "\n",
      "    accuracy                           0.69       108\n",
      "   macro avg       0.67      0.66      0.66       108\n",
      "weighted avg       0.69      0.69      0.69       108\n",
      "\n",
      "RF Random Accuracy: 0.694444\n",
      "RF Random F1score: 0.691584\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.51      0.59        39\n",
      "           1       0.76      0.87      0.81        69\n",
      "\n",
      "    accuracy                           0.74       108\n",
      "   macro avg       0.72      0.69      0.70       108\n",
      "weighted avg       0.73      0.74      0.73       108\n",
      "\n",
      "SVM Accuracy: 0.740741\n",
      "SVM F1score: 0.730436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.26      0.38        39\n",
      "           1       0.69      0.94      0.80        69\n",
      "\n",
      "    accuracy                           0.69       108\n",
      "   macro avg       0.70      0.60      0.59       108\n",
      "weighted avg       0.70      0.69      0.65       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.694444\n",
      "SVM RandomCV F1score: 0.645812\n",
      "{'C': 193.65772332668504, 'class_weight': None, 'gamma': 0.1447794086464392, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63        39\n",
      "           1       0.79      0.78      0.79        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.71      0.71      0.71       108\n",
      "weighted avg       0.73      0.73      0.73       108\n",
      "\n",
      "KNN Accuracy grid search: 0.731481\n",
      "KNN F1score grid search: 0.732201\n",
      "{'leaf_size': 1, 'n_neighbors': 1, 'p': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63        39\n",
      "           1       0.79      0.78      0.79        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.71      0.71      0.71       108\n",
      "weighted avg       0.73      0.73      0.73       108\n",
      "\n",
      "KNN Accuracy: 0.731481\n",
      "KNN F1score: 0.732201\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.44      0.56        39\n",
      "           1       0.74      0.93      0.83        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.76      0.68      0.69       108\n",
      "weighted avg       0.75      0.75      0.73       108\n",
      "\n",
      "RF Accuracy: 0.75\n",
      "RF F1score: 0.728874\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.49      0.59        39\n",
      "           1       0.76      0.91      0.83        69\n",
      "\n",
      "    accuracy                           0.76       108\n",
      "   macro avg       0.76      0.70      0.71       108\n",
      "weighted avg       0.76      0.76      0.74       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.759259\n",
      "RF Random F1score _PARAMS: 0.744015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.60        39\n",
      "           1       0.76      0.88      0.82        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.74      0.70      0.71       108\n",
      "weighted avg       0.75      0.75      0.74       108\n",
      "\n",
      "RF Random Accuracy: 0.75\n",
      "RF Random F1score: 0.738706\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=20)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.49      0.57        39\n",
      "           1       0.75      0.87      0.81        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.71      0.68      0.69       108\n",
      "weighted avg       0.72      0.73      0.72       108\n",
      "\n",
      "SVM Accuracy: 0.731481\n",
      "SVM F1score: 0.719351\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.28      0.39        39\n",
      "           1       0.69      0.90      0.78        69\n",
      "\n",
      "    accuracy                           0.68       108\n",
      "   macro avg       0.65      0.59      0.58       108\n",
      "weighted avg       0.66      0.68      0.64       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.675926\n",
      "SVM RandomCV F1score: 0.637629\n",
      "{'C': 20.2350436780055, 'class_weight': 'balanced', 'gamma': 0.0804797390243771, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50        39\n",
      "           1       0.72      0.70      0.71        69\n",
      "\n",
      "    accuracy                           0.63       108\n",
      "   macro avg       0.60      0.60      0.60       108\n",
      "weighted avg       0.63      0.63      0.63       108\n",
      "\n",
      "KNN Accuracy grid search: 0.62963\n",
      "KNN F1score grid search: 0.631536\n",
      "{'leaf_size': 1, 'n_neighbors': 1, 'p': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.54      0.53        39\n",
      "           1       0.73      0.71      0.72        69\n",
      "\n",
      "    accuracy                           0.65       108\n",
      "   macro avg       0.62      0.62      0.62       108\n",
      "weighted avg       0.65      0.65      0.65       108\n",
      "\n",
      "KNN Accuracy: 0.648148\n",
      "KNN F1score: 0.649959\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.51      0.60        39\n",
      "           1       0.76      0.88      0.82        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.74      0.70      0.71       108\n",
      "weighted avg       0.75      0.75      0.74       108\n",
      "\n",
      "RF Accuracy: 0.75\n",
      "RF F1score: 0.738706\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.54      0.61        39\n",
      "           1       0.77      0.87      0.82        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.73      0.70      0.71       108\n",
      "weighted avg       0.74      0.75      0.74       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.75\n",
      "RF Random F1score _PARAMS: 0.741349\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.49      0.58        39\n",
      "           1       0.75      0.88      0.81        69\n",
      "\n",
      "    accuracy                           0.74       108\n",
      "   macro avg       0.73      0.69      0.69       108\n",
      "weighted avg       0.74      0.74      0.73       108\n",
      "\n",
      "RF Random Accuracy: 0.740741\n",
      "RF Random F1score: 0.727542\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=30)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.51      0.59        39\n",
      "           1       0.76      0.87      0.81        69\n",
      "\n",
      "    accuracy                           0.74       108\n",
      "   macro avg       0.72      0.69      0.70       108\n",
      "weighted avg       0.73      0.74      0.73       108\n",
      "\n",
      "SVM Accuracy: 0.740741\n",
      "SVM F1score: 0.730436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.21      0.31        39\n",
      "           1       0.68      0.94      0.79        69\n",
      "\n",
      "    accuracy                           0.68       108\n",
      "   macro avg       0.67      0.57      0.55       108\n",
      "weighted avg       0.67      0.68      0.62       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.675926\n",
      "SVM RandomCV F1score: 0.616657\n",
      "{'C': 20.2350436780055, 'class_weight': 'balanced', 'gamma': 0.0804797390243771, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.56      0.54        39\n",
      "           1       0.74      0.70      0.72        69\n",
      "\n",
      "    accuracy                           0.65       108\n",
      "   macro avg       0.63      0.63      0.63       108\n",
      "weighted avg       0.66      0.65      0.65       108\n",
      "\n",
      "KNN Accuracy grid search: 0.648148\n",
      "KNN F1score grid search: 0.651478\n",
      "{'leaf_size': 1, 'n_neighbors': 1, 'p': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.62      0.60        39\n",
      "           1       0.78      0.75      0.76        69\n",
      "\n",
      "    accuracy                           0.70       108\n",
      "   macro avg       0.68      0.68      0.68       108\n",
      "weighted avg       0.71      0.70      0.71       108\n",
      "\n",
      "KNN Accuracy: 0.703704\n",
      "KNN F1score: 0.705229\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.54      0.61        39\n",
      "           1       0.77      0.87      0.82        69\n",
      "\n",
      "    accuracy                           0.75       108\n",
      "   macro avg       0.73      0.70      0.71       108\n",
      "weighted avg       0.74      0.75      0.74       108\n",
      "\n",
      "RF Accuracy: 0.75\n",
      "RF F1score: 0.741349\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.41      0.51        39\n",
      "           1       0.73      0.88      0.80        69\n",
      "\n",
      "    accuracy                           0.71       108\n",
      "   macro avg       0.70      0.65      0.65       108\n",
      "weighted avg       0.70      0.71      0.69       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.712963\n",
      "RF Random F1score _PARAMS: 0.692862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.46      0.53        39\n",
      "           1       0.73      0.84      0.78        69\n",
      "\n",
      "    accuracy                           0.70       108\n",
      "   macro avg       0.68      0.65      0.66       108\n",
      "weighted avg       0.69      0.70      0.69       108\n",
      "\n",
      "RF Random Accuracy: 0.703704\n",
      "RF Random F1score: 0.691927\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=40)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 277, 1: 277})\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.58        39\n",
      "           1       0.76      0.83      0.79        69\n",
      "\n",
      "    accuracy                           0.72       108\n",
      "   macro avg       0.70      0.68      0.69       108\n",
      "weighted avg       0.72      0.72      0.72       108\n",
      "\n",
      "SVM Accuracy: 0.722222\n",
      "SVM F1score: 0.716435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.31      0.41        39\n",
      "           1       0.69      0.88      0.78        69\n",
      "\n",
      "    accuracy                           0.68       108\n",
      "   macro avg       0.65      0.60      0.59       108\n",
      "weighted avg       0.66      0.68      0.64       108\n",
      "\n",
      "SVM RandomCV Accuracy: 0.675926\n",
      "SVM RandomCV F1score: 0.643354\n",
      "{'C': 6.961767177320046, 'class_weight': None, 'gamma': 0.06512307303628603, 'kernel': 'rbf'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.62      0.56        39\n",
      "           1       0.76      0.68      0.72        69\n",
      "\n",
      "    accuracy                           0.66       108\n",
      "   macro avg       0.64      0.65      0.64       108\n",
      "weighted avg       0.67      0.66      0.66       108\n",
      "\n",
      "KNN Accuracy grid search: 0.657407\n",
      "KNN F1score grid search: 0.662361\n",
      "{'leaf_size': 1, 'n_neighbors': 1, 'p': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 6 is smaller than n_iter=100. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60        39\n",
      "           1       0.78      0.72      0.75        69\n",
      "\n",
      "    accuracy                           0.69       108\n",
      "   macro avg       0.67      0.68      0.68       108\n",
      "weighted avg       0.70      0.69      0.70       108\n",
      "\n",
      "KNN Accuracy: 0.694444\n",
      "KNN F1score: 0.697904\n",
      "{'n_neighbors': 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.54      0.59        39\n",
      "           1       0.76      0.84      0.80        69\n",
      "\n",
      "    accuracy                           0.73       108\n",
      "   macro avg       0.71      0.69      0.70       108\n",
      "weighted avg       0.72      0.73      0.72       108\n",
      "\n",
      "RF Accuracy: 0.731481\n",
      "RF F1score: 0.724726\n",
      "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Fitting 10 folds for each of 100 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aquar\\anaconda3\\envs\\TF2\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:427: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.46      0.54        39\n",
      "           1       0.74      0.86      0.79        69\n",
      "\n",
      "    accuracy                           0.71       108\n",
      "   macro avg       0.69      0.66      0.66       108\n",
      "weighted avg       0.70      0.71      0.70       108\n",
      "\n",
      "RF Random Accuracy _PARAMS: 0.712963\n",
      "RF Random F1score _PARAMS: 0.699996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.49      0.56        39\n",
      "           1       0.75      0.86      0.80        69\n",
      "\n",
      "    accuracy                           0.72       108\n",
      "   macro avg       0.70      0.67      0.68       108\n",
      "weighted avg       0.71      0.72      0.71       108\n",
      "\n",
      "RF Random Accuracy: 0.722222\n",
      "RF Random F1score: 0.711182\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training/testing sets\n",
    "#X_train_ff_ia, X_test_ff_ia, y_train_ff_ia, y_test_ff_ia = train_test_split(ff_data, y_ia, test_size=0.2, random_state=42, stratify = y_ia)\n",
    "#X_train_ff_aq, X_test_ff_aq, y_train_ff_aq, y_test_ff_aq = train_test_split(ff_data, y_aq, test_size=0.2, random_state=42, stratify = y_aq)\n",
    "#X_train_ff_iv, X_test_ff_iv, y_train_ff_iv, y_test_ff_iv = train_test_split(ff_data, y_iv, test_size=0.2, random_state=42, stratify = y_iv)\n",
    "#X_train_ff_il, X_test_ff_il, y_train_ff_il, y_test_ff_il = train_test_split(ff_data, y_il, test_size=0.2, random_state=42, stratify = y_il)\n",
    "X_train_ff_oq, X_test_ff_oq, y_train_ff_oq, y_test_ff_oq = train_test_split(ff_data, y_oq, test_size=0.2, random_state=42, stratify = y_oq)\n",
    "X_train_ff_sa, X_test_ff_sa, y_train_ff_sa, y_test_ff_sa = train_test_split(ff_data, y_sa, test_size=0.2, random_state=42, stratify = y_sa)\n",
    "  \n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_ff_oq= sc.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = sc.transform(X_test_ff_oq)\n",
    "\n",
    "\n",
    "#principle component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=50)\n",
    "X_train_ff_oq = pca.fit_transform(X_train_ff_oq)\n",
    "X_test_ff_oq = pca.transform(X_test_ff_oq)\n",
    "\n",
    "# number of components\n",
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component i.e. largest absolute value\n",
    "# using LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "initial_feature_names = ff_df.columns\n",
    "# get the names\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n",
    "\n",
    "# using LIST COMPREHENSION HERE AGAIN\n",
    "dic = {'PC{}'.format(i+1): most_important_names[i] for i in range(n_pcs)}\n",
    "\n",
    "# build the dataframe\n",
    "df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_ff_oq, y_train_ff_oq)\n",
    "X_test_resampled, y_test_resampled = X_test_ff_oq, y_test_ff_oq\n",
    "\n",
    "from collections import Counter\n",
    "# summarize observations by class labeL\n",
    "counter = Counter(y_train_resampled)\n",
    "print(counter)\n",
    "\n",
    "ff_randsearch = SVC(gamma= 'auto', random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "rskfcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "loocv = LeaveOneOut()\n",
    "\n",
    "\n",
    "#from sklearn.utils.fixes import loguniform\n",
    "ff_svm_param_grid= {'C': expon(scale=100), 'gamma': expon(scale=.1), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "#ff_svm_param_grid= {'C': reciprocal(1e0, 1e3), 'gamma': reciprocal(1e-3, 1e-2), 'kernel': ['rbf'], 'class_weight':['balanced', None]}\n",
    "\n",
    "ff_randsearch = RandomizedSearchCV(SVC(decision_function_shape='ovo'),param_distributions = ff_svm_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "#ff_randsearch.fit(X_train_ff_il, y_train_ff_il) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"SVM RandomCV Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"SVM RandomCV F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred,average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "#List Hyperparameters that we want to tune.\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "#Convert to dictionary\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "#Create new KNN object\n",
    "knn_2 = KNeighborsClassifier()\n",
    "#Use GridSearch\n",
    "clf = GridSearchCV(knn_2, hyperparameters, cv=10)\n",
    "clf.fit(X_train_resampled, y_train_resampled) \n",
    "clf_pred = clf.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, clf_pred))\n",
    "print(\"KNN Accuracy grid search:\", round(metrics.accuracy_score(y_test_resampled, clf_pred), 6))\n",
    "print(\"KNN F1score grid search:\", round(metrics.f1_score(y_test_resampled, clf_pred, average = 'weighted'), 6))\n",
    "print(clf.best_params_)\n",
    "\n",
    "ff_knn_param_grid= {'n_neighbors': (1,2,3,10, 20,100)}\n",
    "ff_randsearch = RandomizedSearchCV(KNeighborsClassifier(),param_distributions = ff_knn_param_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"KNN Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"KNN F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "print(ff_randsearch.best_params_)\n",
    "\n",
    "ff_randsearch = RandomForestClassifier() \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "ff_randsearchRF = RandomizedSearchCV(RandomForestClassifier(),param_distributions = random_grid, n_iter = 100, n_jobs = -1, cv = 10, random_state = 0, verbose =2) \n",
    "ff_randsearchRF.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearchRF_pred = ff_randsearchRF.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearchRF_pred))\n",
    "print(\"RF Random Accuracy _PARAMS:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearchRF_pred), 6))\n",
    "print(\"RF Random F1score _PARAMS:\", round(metrics.f1_score(y_test_resampled, ff_randsearchRF_pred, average = 'weighted'), 6))\n",
    "\n",
    "\n",
    "ff_randsearch = RandomForestClassifier(max_depth=10, random_state=0) \n",
    "ff_randsearch.fit(X_train_resampled, y_train_resampled) \n",
    "ff_randsearch_pred = ff_randsearch.predict(X_test_resampled)\n",
    "print(metrics.classification_report(y_test_resampled, ff_randsearch_pred))\n",
    "print(\"RF Random Accuracy:\", round(metrics.accuracy_score(y_test_resampled, ff_randsearch_pred), 6))\n",
    "print(\"RF Random F1score:\", round(metrics.f1_score(y_test_resampled, ff_randsearch_pred, average = 'weighted'), 6))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
